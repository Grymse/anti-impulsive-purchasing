{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IAIService import IAIService\n",
    "import google.generativeai as genai\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "            \"temperature\": 0.01,\n",
    "            \"top_p\": 1.0,\n",
    "            \"top_k\": 64,\n",
    "            \"max_output_tokens\": 8192,\n",
    "            \"response_mime_type\": \"text/plain\",\n",
    "            }\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "            model_name=\"gemini-1.5-flash\",\n",
    "            generation_config=generation_config,\n",
    "            # safety_settings = Adjust safety settings\n",
    "            # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
    "            )\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Leftover pizza\n",
    "üçé Food\n",
    "Probably not the most frugal but helpful tip:\n",
    "\n",
    "whenever we order pizza we intentionally order large and just freeze whatever slices we don‚Äôt eat. then when we don‚Äôt have time and are super hungry and busy we just pop the slices we wanna rest in the toaster oven.\n",
    "\n",
    "important to store in freezer safe bags or containers. And try to eat within the first 3 months.\n",
    "\n",
    "We also freeze lasagna as when making it we usually make too much for just 2/4 people. i do the same with rice and it‚Äôs a great kickstarter for quick meals like fried rice or egg drop soups with rice.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "chat_session = model.start_chat(\n",
    "            history=[\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    \"This GPT is designed to help users classify Reddit submissions as either seeking advice(1) or not(0). It analyzes the content, identifies cues like direct questions, requests for help, or phrases indicating advice-seeking behavior. It should distinguish posts that are simply sharing experiences or opinions from those that are actively asking for input, solutions, or guidance. The GPT only ever receives submissions pasted directly from reddit as input. Typically the format would start off with the title, then a flair and finally the selftext. However, this is not always the case.\\nThe GPT is only ever allowed to answer between: 1/0\",\n",
    "                ],\n",
    "                },\n",
    "                {\n",
    "                \"role\": \"model\",\n",
    "                \"parts\": [\n",
    "                    \"Okay, I understand! \\n\\nYou can start pasting Reddit submissions and I'll respond with a \\\"1\\\" if the post is seeking advice and a \\\"0\\\" if it's not.  \\n\\nLet's begin! \\n\",\n",
    "                ],\n",
    "                },\n",
    "            ]\n",
    "            )\n",
    "response = chat_session.send_message(prompt)\n",
    "print(response.text[0] == \"1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
