{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from OpenAIService import OpenAIService\n",
    "import os\n",
    "\n",
    "openai_service = OpenAIService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_SHOPAHOLIC_SYSTEM_PROMPT = \"\"\"This GPT is designed to help the user classify whether the author of an advice-seeking Reddit submission is a shopaholic(1) or not(0). Specifically, the user of the GPT wants to know if the submission's author has any impulsive buying or shopaholic tendencies. The author may not explicitly indicate/admit their problem, but the clues can be hidden between the lines. The GPT can make a judgment call even if the user does not explicitly admit their problem. The GPT should try to be as accurate as possible, and if ever in doubt about submission, it should make its best guess. The GPT only ever receives submissions pasted directly from reddit as input. Typically the format would start off with the title, then a flair and finally the selftext. However, this is not always the case. The GPT is only ever allowed to respond with either: 1/0\"\"\"\n",
    "LOCATION_PROCESSED = 'data_processed'\n",
    "LOCATION_BATCHES = f\"{LOCATION_PROCESSED}/batches\"\n",
    "ALL_SUBMISSIONS_FILENAME = f\"{LOCATION_PROCESSED}/all_submissions_filtered_and_labelled.jsonl\"\n",
    "MAX_BATCH_SIZE = 50_000\n",
    "# MAX_BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total submissions: 260882\n",
      "Advice seeking submissions: 130268\n",
      "Average number of comments: 28.990312279301133\n"
     ]
    }
   ],
   "source": [
    "# load all submissions\n",
    "df = pd.read_json(ALL_SUBMISSIONS_FILENAME, lines=True)\n",
    "print(f\"Total submissions: {len(df)}\")\n",
    "\n",
    "# filter out non-advice seeking submissions\n",
    "df = df[df['is_advice_seeking'] == True]\n",
    "print(f\"Advice seeking submissions: {len(df)}\")\n",
    "\n",
    "# print average number of comments\n",
    "print(f\"Average number of comments: {df['num_comments'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    submission_text = f\"{row['title']}\"\n",
    "\n",
    "    flair = row['link_flair_text']\n",
    "    if not pd.isna(flair):\n",
    "        submission_text += f\"\\n{flair}\"\n",
    "\n",
    "    selftext = row['selftext']\n",
    "    if not pd.isna(selftext):\n",
    "        submission_text += f\"\\n{selftext}\"\n",
    "    \n",
    "    unique_id = f\"{row['subreddit']}_{row['id']}\"\n",
    "    # print(f\"Creating line for {unique_id}\")\n",
    "\n",
    "    line = openai_service.create_line(unique_id, submission_text, system_prompt=SUBMISSION_SHOPAHOLIC_SYSTEM_PROMPT)\n",
    "\n",
    "    all_lines.append(line)\n",
    "\n",
    "# print(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique batching string: db6ffccf\n",
      "Creating batch db6ffccf_0_to_50000 with 50000 line(s)\n",
      "Creating batch db6ffccf_50000_to_100000 with 50000 line(s)\n",
      "Creating batch db6ffccf_100000_to_130268 with 30268 line(s)\n"
     ]
    }
   ],
   "source": [
    "# creating batches\n",
    "batches = {}\n",
    "# generate unique string for batching session\n",
    "unique_batching_string = np.random.bytes(4).hex()\n",
    "print(f\"Unique batching string: {unique_batching_string}\")\n",
    "\n",
    "for i in range(0, len(all_lines), MAX_BATCH_SIZE):\n",
    "    from_index = i\n",
    "    to_index = min(i+MAX_BATCH_SIZE, len(all_lines))\n",
    "    batch_name = f\"{unique_batching_string}_{from_index}_to_{to_index}\"\n",
    "\n",
    "    print(f\"Creating batch {batch_name} with {to_index - from_index} line(s)\")\n",
    "    batch = all_lines[from_index:to_index]\n",
    "\n",
    "    batches[batch_name] = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save batches to disk\n",
    "for batch_name, lines in batches.items():\n",
    "    if not os.path.exists(LOCATION_BATCHES):\n",
    "        os.makedirs(LOCATION_BATCHES)\n",
    "\n",
    "        # create batch folder\n",
    "    batch_folder = f\"{LOCATION_BATCHES}/{unique_batching_string}\"\n",
    "    if not os.path.exists(batch_folder):\n",
    "        os.makedirs(batch_folder)\n",
    "\n",
    "    # save lines to disk\n",
    "    batch_filename = f\"{batch_folder}/{batch_name}.jsonl\"\n",
    "    with open(batch_filename, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
